#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸‹è½½é“¾æ¥æå–å™¨
ä»APIæŠ“å–æ—¥å¿—ä¸­æå–çœŸå®çš„ä¸‹è½½é“¾æ¥å¹¶éªŒè¯å¯ç”¨æ€§
"""

import json
import os
import requests
import urllib.parse
from datetime import datetime
from colorama import init, Fore, Style
import re

# åˆå§‹åŒ–colorama
init()

class DownloadLinkExtractor:
    def __init__(self):
        self.download_links = []
        
    def extract_from_logs(self, log_directory="logs"):
        """ä»æ—¥å¿—æ–‡ä»¶ä¸­æå–ä¸‹è½½é“¾æ¥"""
        print(f"{Fore.GREEN}ğŸ” å¼€å§‹ä»æ—¥å¿—ä¸­æå–ä¸‹è½½é“¾æ¥{Style.RESET_ALL}")
        print(f"{Fore.CYAN}{'='*60}{Style.RESET_ALL}")
        
        if not os.path.exists(log_directory):
            print(f"{Fore.RED}âŒ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_directory}{Style.RESET_ALL}")
            return []
        
        # æŸ¥æ‰¾æ‰€æœ‰JSONæ—¥å¿—æ–‡ä»¶
        json_files = [f for f in os.listdir(log_directory) if f.startswith('api_requests_') and f.endswith('.json')]
        
        if not json_files:
            print(f"{Fore.YELLOW}âš ï¸  æœªæ‰¾åˆ°APIè¯·æ±‚æ—¥å¿—æ–‡ä»¶{Style.RESET_ALL}")
            return []
        
        for json_file in json_files:
            file_path = os.path.join(log_directory, json_file)
            print(f"\nğŸ“„ åˆ†ææ–‡ä»¶: {json_file}")
            self._process_log_file(file_path)
        
        return self.download_links
    
    def _process_log_file(self, file_path):
        """å¤„ç†å•ä¸ªæ—¥å¿—æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            for entry in data:
                if 'response' in entry and 'body' in entry['response']:
                    self._extract_download_urls(entry)
                    
        except Exception as e:
            print(f"{Fore.RED}âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}{Style.RESET_ALL}")
    
    def _extract_download_urls(self, entry):
        """ä»å“åº”ä¸­æå–ä¸‹è½½URL"""
        try:
            response_body = entry['response']['body']
            request_url = entry['request']['url']
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é˜¿é‡Œäº‘ç›˜APIå“åº”
            if ('aliyun' in request_url and 'api.php' in request_url) or \
               ('aliyundrive' in str(response_body)):
                
                download_url = None
                file_info = {}
                
                # è§£æJSONå“åº”
                if isinstance(response_body, dict):
                    # ç›´æ¥æ˜¯å­—å…¸æ ¼å¼
                    if 'url' in response_body:
                        download_url = response_body['url']
                        file_info = response_body
                elif isinstance(response_body, str):
                    # å°è¯•è§£æJSONå­—ç¬¦ä¸²
                    try:
                        json_data = json.loads(response_body)
                        if 'url' in json_data:
                            download_url = json_data['url']
                            file_info = json_data
                    except:
                        # å¯èƒ½æ˜¯HTMLæˆ–å…¶ä»–æ ¼å¼ï¼Œå°è¯•æ­£åˆ™æå–
                        url_pattern = r'https://[^"\'>\s]+aliyundrive\.net[^"\'>\s]*'
                        urls = re.findall(url_pattern, response_body)
                        if urls:
                            download_url = urls[0]
                
                if download_url and 'aliyundrive.net' in download_url:
                    link_info = {
                        'timestamp': entry['request']['timestamp'],
                        'request_url': request_url,
                        'download_url': download_url,
                        'file_info': file_info,
                        'query_params': entry['request'].get('query_params', {})
                    }
                    
                    self.download_links.append(link_info)
                    self._display_found_link(link_info)
                    
        except Exception as e:
            pass  # è·³è¿‡æ— æ³•è§£æçš„å“åº”
    
    def _display_found_link(self, link_info):
        """æ˜¾ç¤ºæ‰¾åˆ°çš„ä¸‹è½½é“¾æ¥"""
        print(f"\n{Fore.GREEN}âœ… å‘ç°ä¸‹è½½é“¾æ¥{Style.RESET_ALL}")
        print(f"â° æ—¶é—´: {link_info['timestamp']}")
        print(f"ğŸ”— è¯·æ±‚URL: {link_info['request_url']}")
        
        # æ˜¾ç¤ºæŸ¥è¯¢å‚æ•°
        if link_info['query_params']:
            print(f"ğŸ“‹ è¯·æ±‚å‚æ•°:")
            for key, value in link_info['query_params'].items():
                print(f"   {key}: {value}")
        
        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        if link_info['file_info']:
            if 'type' in link_info['file_info']:
                print(f"ğŸ“ æ–‡ä»¶ç±»å‹: {link_info['file_info']['type']}")
            if 'delfile' in link_info['file_info']:
                print(f"ğŸ—‘ï¸  åˆ é™¤çŠ¶æ€: {link_info['file_info']['delfile']}")
        
        # æ˜¾ç¤ºä¸‹è½½é“¾æ¥ï¼ˆæˆªæ–­æ˜¾ç¤ºï¼‰
        url = link_info['download_url']
        if len(url) > 100:
            display_url = url[:50] + "..." + url[-50:]
        else:
            display_url = url
        print(f"ğŸ“¥ ä¸‹è½½é“¾æ¥: {display_url}")
        print(f"{Fore.CYAN}{'-'*60}{Style.RESET_ALL}")
    
    def verify_links(self):
        """éªŒè¯ä¸‹è½½é“¾æ¥çš„å¯ç”¨æ€§"""
        print(f"\n{Fore.YELLOW}ğŸ” å¼€å§‹éªŒè¯ä¸‹è½½é“¾æ¥{Style.RESET_ALL}")
        print(f"{Fore.CYAN}{'='*60}{Style.RESET_ALL}")
        
        for i, link_info in enumerate(self.download_links):
            print(f"\nğŸ“ éªŒè¯é“¾æ¥ {i+1}/{len(self.download_links)}")
            self._verify_single_link(link_info)
    
    def _verify_single_link(self, link_info):
        """éªŒè¯å•ä¸ªä¸‹è½½é“¾æ¥"""
        url = link_info['download_url']
        
        try:
            # å‘é€HEADè¯·æ±‚æ£€æŸ¥é“¾æ¥çŠ¶æ€
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            print(f"ğŸ”„ æ£€æŸ¥é“¾æ¥çŠ¶æ€...")
            response = requests.head(url, headers=headers, timeout=10, allow_redirects=True)
            
            print(f"ğŸ“Š çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                print(f"{Fore.GREEN}âœ… é“¾æ¥æœ‰æ•ˆï¼{Style.RESET_ALL}")
                
                # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
                if 'Content-Length' in response.headers:
                    size = int(response.headers['Content-Length'])
                    size_mb = size / (1024 * 1024)
                    print(f"ğŸ“ æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")
                
                if 'Content-Type' in response.headers:
                    print(f"ğŸ“„ æ–‡ä»¶ç±»å‹: {response.headers['Content-Type']}")
                
                if 'Content-Disposition' in response.headers:
                    disposition = response.headers['Content-Disposition']
                    # æå–æ–‡ä»¶å
                    filename_match = re.search(r'filename[^;=\n]*=(([\'"]).*?\2|[^;\n]*)', disposition)
                    if filename_match:
                        filename = filename_match.group(1).strip('"\'')
                        # URLè§£ç æ–‡ä»¶å
                        try:
                            filename = urllib.parse.unquote(filename)
                        except:
                            pass
                        print(f"ğŸ“ æ–‡ä»¶å: {filename}")
                        
            elif response.status_code == 403:
                print(f"{Fore.YELLOW}âš ï¸  é“¾æ¥å·²è¿‡æœŸæˆ–æ— æƒé™è®¿é—®{Style.RESET_ALL}")
            elif response.status_code == 404:
                print(f"{Fore.RED}âŒ æ–‡ä»¶ä¸å­˜åœ¨{Style.RESET_ALL}")
            else:
                print(f"{Fore.YELLOW}âš ï¸  çŠ¶æ€ç : {response.status_code}{Style.RESET_ALL}")
                
        except requests.exceptions.Timeout:
            print(f"{Fore.YELLOW}â° è¯·æ±‚è¶…æ—¶{Style.RESET_ALL}")
        except requests.exceptions.RequestException as e:
            print(f"{Fore.RED}âŒ è¯·æ±‚å¤±è´¥: {str(e)}{Style.RESET_ALL}")
    
    def save_links_to_file(self, filename="extracted_download_links.json"):
        """ä¿å­˜æå–çš„é“¾æ¥åˆ°æ–‡ä»¶"""
        if not self.download_links:
            print(f"{Fore.YELLOW}âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä¸‹è½½é“¾æ¥{Style.RESET_ALL}")
            return
        
        output_data = {
            'extracted_time': datetime.now().isoformat(),
            'total_links': len(self.download_links),
            'links': self.download_links
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            print(f"\n{Fore.GREEN}ğŸ’¾ é“¾æ¥å·²ä¿å­˜åˆ°: {filename}{Style.RESET_ALL}")
            print(f"ğŸ“Š æ€»å…±æå–äº† {len(self.download_links)} ä¸ªä¸‹è½½é“¾æ¥")
            
        except Exception as e:
            print(f"{Fore.RED}âŒ ä¿å­˜å¤±è´¥: {str(e)}{Style.RESET_ALL}")
    
    def get_direct_download_url(self, aliyun_api_url):
        """ç›´æ¥ä»é˜¿é‡Œäº‘APIè·å–ä¸‹è½½é“¾æ¥"""
        print(f"\n{Fore.BLUE}ğŸš€ å°è¯•ç›´æ¥è·å–ä¸‹è½½é“¾æ¥{Style.RESET_ALL}")
        print(f"ğŸ”— API URL: {aliyun_api_url}")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Linux; Android 10) AppleWebKit/537.36',
                'Accept': 'application/json, text/plain, */*'
            }
            
            response = requests.get(aliyun_api_url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    if 'url' in data:
                        print(f"{Fore.GREEN}âœ… æˆåŠŸè·å–ä¸‹è½½é“¾æ¥ï¼{Style.RESET_ALL}")
                        return data['url']
                    else:
                        print(f"{Fore.YELLOW}âš ï¸  å“åº”ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸‹è½½é“¾æ¥{Style.RESET_ALL}")
                        print(f"å“åº”å†…å®¹: {json.dumps(data, ensure_ascii=False, indent=2)}")
                except:
                    print(f"{Fore.YELLOW}âš ï¸  å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼{Style.RESET_ALL}")
                    print(f"å“åº”å†…å®¹: {response.text[:500]}...")
            else:
                print(f"{Fore.RED}âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}{Style.RESET_ALL}")
                
        except Exception as e:
            print(f"{Fore.RED}âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}{Style.RESET_ALL}")
        
        return None

def main():
    extractor = DownloadLinkExtractor()
    
    # ä»æ—¥å¿—ä¸­æå–é“¾æ¥
    links = extractor.extract_from_logs()
    
    if links:
        # éªŒè¯é“¾æ¥
        extractor.verify_links()
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        extractor.save_links_to_file()
        
        print(f"\n{Fore.GREEN}ğŸ‰ æå–å®Œæˆï¼{Style.RESET_ALL}")
        print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(links)} ä¸ªä¸‹è½½é“¾æ¥")
        
        # æ˜¾ç¤ºæœ€æ–°çš„ä¸€ä¸ªé“¾æ¥
        if links:
            latest_link = links[-1]
            print(f"\n{Fore.BLUE}ğŸ”— æœ€æ–°çš„ä¸‹è½½é“¾æ¥ï¼š{Style.RESET_ALL}")
            print(f"â° æ—¶é—´: {latest_link['timestamp']}")
            url = latest_link['download_url']
            if len(url) > 150:
                print(f"ğŸ“¥ é“¾æ¥: {url[:75]}...{url[-75:]}")
            else:
                print(f"ğŸ“¥ é“¾æ¥: {url}")
    else:
        print(f"{Fore.YELLOW}âš ï¸  æœªæ‰¾åˆ°ä»»ä½•ä¸‹è½½é“¾æ¥{Style.RESET_ALL}")
        print("è¯·ç¡®ä¿ï¼š")
        print("1. å·²è¿è¡Œä»£ç†æœåŠ¡å™¨å¹¶æŠ“å–äº†APIè¯·æ±‚")
        print("2. åœ¨APKä¸­æ‰§è¡Œäº†ä¸‹è½½æ“ä½œ")
        print("3. logsç›®å½•ä¸­å­˜åœ¨APIè¯·æ±‚æ—¥å¿—æ–‡ä»¶")

if __name__ == "__main__":
    main() 